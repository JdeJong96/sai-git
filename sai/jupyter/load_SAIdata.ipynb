{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f910d0a-62ea-4477-b688-781da970f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from kerchunk.netCDF3 import NetCDF3ToZarr\n",
    "from kerchunk.combine import MultiZarrToZarr\n",
    "import numpy as np\n",
    "import dask\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bc6ed9f-e885-427b-9923-ba84ace1ed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_mfdataset(filepaths: list[str], ncstore_dir: str='~/kerchunk', **kwargs):\n",
    "    \"\"\"a faster alternative to xr.open_mfdataset using kerchunk\n",
    "    \n",
    "    This function uses kerchunk to create an NC_STORE reference file,\n",
    "    that instructs the program how to read the netCDF files efficiently. \n",
    "    Coordinates must be consistent throughout all files.\n",
    "    The NC_STORE is saved after first use, and will be read on each \n",
    "    subsequent usage of this function.\n",
    "    \n",
    "    Parameters:\n",
    "    filepaths : str or list[str]\n",
    "        (list of) netCDF file names, may contain wild cards\n",
    "    ncstore_dir: Pathlike\n",
    "        Path where NC_STORE reference files will be saved\n",
    "    kwargs: dict\n",
    "        any additional keyword arguments are passed on to xr.open_dataset\n",
    "        \n",
    "    Returns: xr.Dataset\n",
    "        a Dataset instance containing all the netCDF data\n",
    "        \n",
    "    v0.0\n",
    "    \"\"\"\n",
    "    \n",
    "    # make sorted list of absolute filepaths\n",
    "    if isinstance(filepaths, str):\n",
    "        filepaths = glob.glob(filepaths)\n",
    "    filepaths = sorted([os.path.abspath(fp) for fp in filepaths])\n",
    "    if len(filepaths) == 1: # use xr.open_dataset directly if there is one file\n",
    "        return xr.open_dataset(filepaths[0], **kwargs)\n",
    "    \n",
    "    # set default keyword arguments for xr.open_dataset on NC_STORE file\n",
    "    default_kw = {'engine':'kerchunk', 'storage_options':{'target_protocol':'file'}}\n",
    "    for (k,v) in default_kw.items():\n",
    "        if k in kwargs:\n",
    "            print(f'open_mfdataset(): ignoring keyword {k}')\n",
    "        kwargs[k] = v\n",
    "    \n",
    "    # create NC_STORE filename from netCDF filename, including timestamp\n",
    "    # of first and last file. Open and return dataset if the file already exists\n",
    "    ncstore_dir = os.path.expanduser(ncstore_dir)\n",
    "    timestr = lambda i: os.path.basename(filepaths[i]).split('.')[-2] # timestamp\n",
    "    ncstorefile = (os.path.basename(filepaths[0])\n",
    "                   .replace(timestr(0),f\"{timestr(0)}_{timestr(-1)}\")\n",
    "                   .replace('.nc','.json'))\n",
    "    ncstore_path = os.path.join(ncstore_dir, ncstorefile)\n",
    "    if not os.path.exists(ncstore_dir):\n",
    "        os.mkdir(ncstore_dir)\n",
    "    elif os.path.exists(ncstore_path):\n",
    "        print(f\"Reading combined kerchunk reference file {ncstore_path}\")\n",
    "        return xr.open_dataset(ncstore_path, **kwargs)\n",
    "    \n",
    "    # make new NC_STORE data\n",
    "    filebag = dask.bag.from_sequence(filepaths, npartitions=None)\n",
    "    reffiles = (filebag.map(NetCDF3ToZarr, inline_threshold=0, max_chunk_size=0)\n",
    "                .map(lambda z: z.translate()).compute())\n",
    "    mzz = MultiZarrToZarr(reffiles, concat_dims=['time'], coo_map={'time':'cf:time'})\n",
    "    \n",
    "    # write NC_STORE data and return opened dataset\n",
    "    with open(f\"{ncstore_path}\", \"wb\") as f:\n",
    "       print(f\"Writing combined kerchunk reference file {ncstore_path}\")\n",
    "       f.write(json.dumps(mzz.translate()).encode())\n",
    "    \n",
    "    return xr.open_dataset(ncstore_path, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10e863ef-9ed9-45ca-994d-2dc7fbb3d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Case:\n",
    "    '''Class for finding netCDF files in all CESM1.0.4 SAI and control experiments.\n",
    "    \n",
    "    General workflow:\n",
    "    >> from load_SAIdata import Case\n",
    "    >> files = Case(tag).select(comp,stream).files\n",
    "\n",
    "    If you do not know which tag, comp and stream you need:\n",
    "    >> from load_SAIdata import Case\n",
    "    >> print(Case.cases) => pick a tag\n",
    "    >> mycase = Case(<your tag here>)\n",
    "    >> print(mycase) => pick a model component and filestream\n",
    "    >> mydata = mycase.select(<mycomp>,<mystream>)\n",
    "    >> print(mydata) => check time step\n",
    "    >> mydata.open_mfdataset()\n",
    "    '''\n",
    "\n",
    "    # general info and case overview\n",
    "    comps = {'atm':'cam2','ocn':'pop','lnd':'clm2','ice':'cice'} # model components\n",
    "    DIR1 = '/projects/0/prace_imau/prace_2013081679/cesm1_0_4' # root directory SAI\n",
    "    DIR2 = '/projects/0/nwo2021025/archive' # root directory control\n",
    "    cases = {\n",
    "        'lres.spinup':f'{DIR1}/f09_g16/spinup_pd_maxcores_f09_g16/OUTPUT', # 200-300 => 2000-2100\n",
    "        'lres.sai20':f'{DIR2}/lres_b.e10.B2000_CAM5.f09_g16.feedforward.001',\n",
    "        'lres.sai':f'{DIR2}/lres_b.e10.B2000_CAM5.f09_g16.feedforward_2050.001',\n",
    "        'mres.cnt':f'{DIR1}/f05_t12/rcp8.5_co2_f05_t12', # 2000-2100\n",
    "        'mres.sai':f'{DIR2}/mres_b.e10.B2000_CAM5.f05_t12.001', # 2045-2100\n",
    "        'hres.ref.1':f'{DIR1}/f02_t12/b.e10.B_RCP8.5_CO2_CAM5.f02_t12.started_2002-12.001/OUTPUT',\n",
    "        'hres.ref.2':f'{DIR1}/f02_t12/b.e10.B_RCP8.5_CO2_CAM5.f02_t12.started_2002-12.002/OUTPUT',\n",
    "        'hres.ref.3':f'{DIR1}/f02_t12/b.e10.B_RCP8.5_CO2_CAM5.f02_t12.started_2002-12.003/OUTPUT',\n",
    "        'hres.ref.4':f'{DIR1}/f02_t12/b.e10.B_RCP8.5_CO2_CAM5.f02_t12.started_2002-12.004/OUTPUT',\n",
    "        'hres.ref.5':f'{DIR1}/f02_t12/b.e10.B_RCP8.5_CO2_CAM5.f02_t12.started_2002-12.005/OUTPUT',\n",
    "        'hres.ref.6':f'{DIR2}/hres_b.e10.B2000_CAM5.f02_t12.started_2002-12_without_SAI.001', # additional run with 6hrly 3D output\n",
    "        'hres.cnt.1':f'{DIR1}/f02_t12/b.e10.B_RCP8.5_CO2_CAM5.f02_t12.started_2092-12.001/OUTPUT',\n",
    "        'hres.cnt.2':f'{DIR1}/f02_t12/b.e10.B_RCP8.5_CO2_CAM5.f02_t12.started_2092-12.002/OUTPUT',\n",
    "        'hres.cnt.3':f'{DIR1}/f02_t12/b.e10.B_RCP8.5_CO2_CAM5.f02_t12.started_2092-12.003/OUTPUT',\n",
    "        'hres.cnt.4':f'{DIR1}/f02_t12/b.e10.B_RCP8.5_CO2_CAM5.f02_t12.started_2092-12.004/OUTPUT',\n",
    "        'hres.cnt.5':f'{DIR1}/f02_t12/b.e10.B_RCP8.5_CO2_CAM5.f02_t12.started_2092-12.005/OUTPUT',\n",
    "        'hres.cnt.6':f'{DIR2}/hres_b.e10.B2000_CAM5.f02_t12.started_2092-12_without_SAI.001', # additional run with 6hrly 3D output\n",
    "        'hres.sai.1':f'{DIR2}/hres_b.e10.B2000_CAM5.f02_t12.started_2092-12.001',\n",
    "        'hres.sai.2':f'{DIR2}/hres_b.e10.B2000_CAM5.f02_t12.started_2092-12.002',\n",
    "        'hres.sai.3':f'{DIR2}/hres_b.e10.B2000_CAM5.f02_t12.started_2092-12.003',\n",
    "        'hres.sai.4':f'{DIR2}/hres_b.e10.B2000_CAM5.f02_t12.started_2092-12.004',\n",
    "        'hres.sai.5':f'{DIR2}/hres_b.e10.B2000_CAM5.f02_t12.started_2092-12.005',\n",
    "        'hres.sai.6':f'{DIR2}/hres_b.e10.B2000_CAM5.f02_t12.started_2092-12.006', # additional run with 6hrly 3D output\n",
    "    }\n",
    "\n",
    "    \n",
    "    def __init__(self, tag):\n",
    "        '''Initialize a specific case, identified by its tag.'''\n",
    "        self.tag = tag          # tag, e.g. hres.sai.1\n",
    "        self.directory = self.cases[tag] # casedirectory\n",
    "        if not os.path.isdir(self.directory):\n",
    "            print(f\"no directory: {self.directory}\")\n",
    "        self.name = os.path.basename(self.directory.rstrip('/OUTPUT'))  # casename\n",
    "        self.model_component = None\n",
    "        self.file_stream = None\n",
    "        self.files = self._group_ncFiles()\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        '''Always prints case tag and name. Additionally:\n",
    "          before Case.select(): available file streams for each model component\n",
    "          after Case.select(): additional time info (read first netCDF)\n",
    "        '''\n",
    "        msg = f'{self.tag} -- {self.name}'\n",
    "        if isinstance(self.files, list): # after Case.select()\n",
    "            msg += f'\\nmodel component: {self.model_component}\\nfile stream: {self.file_stream}'\n",
    "            msg += f'\\nfirst file: {self.files[0]}\\nlast file: {self.files[-1]}\\nnumber of files: {len(self.files)}'\n",
    "            msg += self._nc_info()\n",
    "        elif isinstance(self.files, dict): # before Case.select()\n",
    "            msg += f'\\ndirectory: {self.directory}'\n",
    "            for cmp in self.files:\n",
    "                msg += f'\\n{cmp}: {list(self.files[cmp])}'\n",
    "        return msg\n",
    "\n",
    "\n",
    "    def select(self, comp, stream):\n",
    "        '''Select model component and output stream'''\n",
    "        self.model_component = comp\n",
    "        self.file_stream = stream\n",
    "        self.files = self.files[comp][stream]\n",
    "        return self\n",
    "\n",
    "\n",
    "    def open_mfdataset(self, *args, **kwargs):\n",
    "        '''Open netCDF files, wrapper for load_SAIdata.open_mfdataset'''\n",
    "        assert isinstance(self.files, list), 'attempted to open dataset without selecting a model component and file stream'\n",
    "        return open_mfdataset(self.files, *args, **kwargs)\n",
    "\n",
    "\n",
    "    def _nc_info(self):\n",
    "        '''Open a netCDF file and return some basic info'''\n",
    "        try:\n",
    "            with xr.open_dataset(self.files[0]) as ds:\n",
    "                msg = f'\\nsteps in first file: {len(ds.time)}'\n",
    "                if hasattr(ds.time, 'bounds'):\n",
    "                    tbounds = ds[ds.time.bounds]\n",
    "                    bnd_dim = tbounds.dims[tbounds.shape.index(2)]\n",
    "                    step = tbounds.isel(time=0).diff(bnd_dim).dt.total_seconds().item()\n",
    "                    msg += '\\ntime step in first file: ' + (f'{step/3600:.1f}H' if step<86400 else f'{step/86400:.1f}D')\n",
    "                else:\n",
    "                    msg += f'\\ntime in first file: {ds.time[:3].values}...'\n",
    "        except Exception as e:\n",
    "            msg = f'Could not fetch additional data from first file due to...\\n{e}'\n",
    "        return msg\n",
    "\n",
    "\n",
    "    def _group_ncFiles(self):\n",
    "        '''Find all netCDF files and group by model component and file stream.'''\n",
    "        result = {cmp:{} for cmp in self.comps.keys()}\n",
    "        ncFiles = [os.path.join(root,file) for root,dirs,files in os.walk(top=self.directory) for file in files if file.endswith('.nc')]\n",
    "        for file in ncFiles:\n",
    "            comp, stream = self._process_filename(file)\n",
    "            if (comp in result) and (stream not in result[comp]) and (stream[0]=='h'): # filter restarts/initial files\n",
    "                result[comp][stream] = [file]\n",
    "            elif (comp in result) and (stream in result[comp]):\n",
    "                result[comp][stream].append(file)\n",
    "        for cmp in result:\n",
    "            result[cmp] = dict(sorted(result[cmp].items())) # sort streams\n",
    "            for stream in result[cmp]:\n",
    "                result[cmp][stream] = np.sort(result[cmp][stream]).tolist() # sort files\n",
    "        return result\n",
    "\n",
    "\n",
    "    def _process_filename(self, fname):\n",
    "        '''Read filename \"fname\" and return model component and file stream.'''\n",
    "        parts = os.path.basename(fname).split('.')\n",
    "        mods = list(self.comps.values()) # model components [cam2, pop, clm2, cice]\n",
    "        try: # find model component in name\n",
    "            imod = [(m in parts) for m in mods].index(True)\n",
    "        except ValueError:\n",
    "            return None, None\n",
    "        comp = list(self.comps)[imod] # general component name [atm, ocn, lnd, ice]\n",
    "        im = parts.index(mods[imod])\n",
    "        is0 = im + 1 # stream starts right after model component\n",
    "        for is1 in range(is0,len(parts)): # some filenames have multiple stream parts, e.g. (pop).h.nday1\n",
    "            if parts[is1][0].isnumeric():\n",
    "                break # loop until first numeric part (date) or last part (nc)\n",
    "        if re.match('^avg[0-9]{4}$', parts[is1-1]):\n",
    "            parts[is1-1] = 'avgYYYY' # group yearly averages\n",
    "        stream = '.'.join(parts[is0:is1])\n",
    "        return comp, stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020a7d14-dd8b-4272-8ea2-c58fb372935d",
   "metadata": {},
   "source": [
    "To Do:  \n",
    "-Test for every set of files if streams are complete  \n",
    "-Add support for strataero files  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0edf86-3b16-4ca5-bc9c-1c556335d7d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
