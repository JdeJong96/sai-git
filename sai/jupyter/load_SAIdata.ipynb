{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f910d0a-62ea-4477-b688-781da970f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from kerchunk.netCDF3 import NetCDF3ToZarr\n",
    "from kerchunk.combine import MultiZarrToZarr\n",
    "import numpy as np\n",
    "import dask\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bc6ed9f-e885-427b-9923-ba84ace1ed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_mfdataset(filepaths: list[str], ncstore_dir: str='~/kerchunk', **kwargs):\n",
    "    \"\"\"a faster alternative to xr.open_mfdataset using kerchunk\n",
    "    \n",
    "    This function uses kerchunk to create an NC_STORE reference file,\n",
    "    that instructs the program how to read the netCDF files efficiently. \n",
    "    Coordinates must be consistent throughout all files.\n",
    "    The NC_STORE is saved after first use, and will be read on each \n",
    "    subsequent usage of this function.\n",
    "    \n",
    "    Parameters:\n",
    "    filepaths : str or list[str]\n",
    "        (list of) netCDF file names, may contain wild cards\n",
    "    ncstore_dir: Pathlike\n",
    "        Path where NC_STORE reference files will be saved\n",
    "    kwargs: dict\n",
    "        any additional keyword arguments are passed on to xr.open_dataset\n",
    "        \n",
    "    Returns: xr.Dataset\n",
    "        a Dataset instance containing all the netCDF data\n",
    "        \n",
    "    v0.0\n",
    "    \"\"\"\n",
    "    \n",
    "    # make sorted list of absolute filepaths\n",
    "    if isinstance(filepaths, str):\n",
    "        filepaths = glob.glob(filepaths)\n",
    "    filepaths = sorted([os.path.abspath(fp) for fp in filepaths])\n",
    "    if len(filepaths) == 1: # use xr.open_dataset directly if there is one file\n",
    "        return xr.open_dataset(filepaths[0], **kwargs)\n",
    "    \n",
    "    # set default keyword arguments for xr.open_dataset on NC_STORE file\n",
    "    default_kw = {'engine':'kerchunk', 'storage_options':{'target_protocol':'file'}}\n",
    "    for (k,v) in default_kw.items():\n",
    "        if k in kwargs:\n",
    "            print(f'open_mfdataset(): ignoring keyword {k}')\n",
    "        kwargs[k] = v\n",
    "    \n",
    "    # create NC_STORE filename from netCDF filename, including timestamp\n",
    "    # of first and last file. Open and return dataset if the file already exists\n",
    "    ncstore_dir = os.path.expanduser(ncstore_dir)\n",
    "    timestr = lambda i: os.path.basename(filepaths[i]).split('.')[-2] # timestamp\n",
    "    ncstorefile = (os.path.basename(filepaths[0])\n",
    "                   .replace(timestr(0),f\"{timestr(0)}_{timestr(-1)}\")\n",
    "                   .replace('.nc','.json'))\n",
    "    ncstore_path = os.path.join(ncstore_dir, ncstorefile)\n",
    "    if not os.path.exists(ncstore_dir):\n",
    "        os.mkdir(ncstore_dir)\n",
    "    elif os.path.exists(ncstore_path):\n",
    "        print(f\"Reading combined kerchunk reference file {ncstore_path}\")\n",
    "        return xr.open_dataset(ncstore_path, **kwargs)\n",
    "    \n",
    "    # make new NC_STORE data\n",
    "    filebag = dask.bag.from_sequence(filepaths, npartitions=None)\n",
    "    reffiles = (filebag.map(NetCDF3ToZarr, inline_threshold=0, max_chunk_size=0)\n",
    "                .map(lambda z: z.translate()).compute())\n",
    "    mzz = MultiZarrToZarr(reffiles, concat_dims=['time'], coo_map={'time':'cf:time'})\n",
    "    \n",
    "    # write NC_STORE data and return opened dataset\n",
    "    with open(f\"{ncstore_path}\", \"wb\") as f:\n",
    "       print(f\"Writing combined kerchunk reference file {ncstore_path}\")\n",
    "       f.write(json.dumps(mzz.translate()).encode())\n",
    "    \n",
    "    return xr.open_dataset(ncstore_path, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10e863ef-9ed9-45ca-994d-2dc7fbb3d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cases:\n",
    "    '''Finding and opening netCDF files in all CESM1.0.4 SAI and control experiments.\n",
    "\n",
    "    Class methods:\n",
    "        select(comp, stream): select a specific model component and file stream\n",
    "        open_mfdataset: open files with load_SAIdata.open_mfdataset (run after select()!)\n",
    "    \n",
    "    Class data:\n",
    "        cases: mapping from case tags to absolute case directories\n",
    "        comps: mapping from CESM component names to their module names\n",
    "        files: before select(): dict of all netCDF files in case directory\n",
    "               after select(): list of all netCDF files in selection\n",
    "    \n",
    "    Examples:\n",
    "    Easy opening of datasets:\n",
    "        >> from load_SAIdata import Case\n",
    "        >> ds = Case(tag).select(comp,stream).open_mfdataset(*args, **kwargs)\n",
    "\n",
    "    If tag, comp and stream are undetermined:\n",
    "        >> from load_SAIdata import Case\n",
    "        >> print(Case.cases)     # pick a tag\n",
    "        >> mycase = Case(<tag>)\n",
    "        >> print(mycase)         # pick a model component and filestream\n",
    "        >> mydata = mycase.select(<comp>,<stream>)\n",
    "        >> print(mydata)         # check time info from first file\n",
    "        >> print(mydata.files)   # check files before opening \n",
    "        >> mydata.open_mfdataset() #\n",
    "    '''\n",
    "\n",
    "    # general info and case overview\n",
    "    comps = {'atm':'cam2','ocn':'pop','lnd':'clm2','ice':'cice', 'strataero':'strataero', 'volcaero':'volcaero'} # model components\n",
    "    DIR1 = '/projects/0/prace_imau/prace_2013081679/cesm1_0_4' # root directory SAI\n",
    "    DIR2 = '/projects/0/nwo2021025/archive' # root directory control\n",
    "    cases = {\n",
    "        'lres.spinup':f'{DIR1}/f09_g16/spinup_pd_maxcores_f09_g16/OUTPUT', # 200-300 => 2000-2100\n",
    "        'lres.sai20':f'{DIR2}/lres_b.e10.B2000_CAM5.f09_g16.feedforward.001',\n",
    "        'lres.sai':f'{DIR2}/lres_b.e10.B2000_CAM5.f09_g16.feedforward_2050.001',\n",
    "        'mres.cnt':f'{DIR1}/f05_t12/rcp8.5_co2_f05_t12', # 2000-2100\n",
    "        'mres.sai':f'{DIR2}/mres_b.e10.B2000_CAM5.f05_t12.001', # 2045-2100\n",
    "        'hres.ref.1':f'{DIR1}/f02_t12/b.e10.B_RCP8.5_CO2_CAM5.f02_t12.started_2002-12.001/OUTPUT',\n",
    "        'hres.ref.2':f'{DIR1}/f02_t12/b.e10.B_RCP8.5_CO2_CAM5.f02_t12.started_2002-12.002/OUTPUT',\n",
    "        'hres.ref.3':f'{DIR1}/f02_t12/b.e10.B_RCP8.5_CO2_CAM5.f02_t12.started_2002-12.003/OUTPUT',\n",
    "        'hres.ref.4':f'{DIR1}/f02_t12/b.e10.B_RCP8.5_CO2_CAM5.f02_t12.started_2002-12.004/OUTPUT',\n",
    "        'hres.ref.5':f'{DIR1}/f02_t12/b.e10.B_RCP8.5_CO2_CAM5.f02_t12.started_2002-12.005/OUTPUT',\n",
    "        'hres.ref.6':f'{DIR2}/hres_b.e10.B2000_CAM5.f02_t12.started_2002-12_without_SAI.001', # additional run with 6hrly 3D output\n",
    "        'hres.cnt.1':f'{DIR1}/f02_t12/b.e10.B_RCP8.5_CO2_CAM5.f02_t12.started_2092-12.001/OUTPUT',\n",
    "        'hres.cnt.2':f'{DIR1}/f02_t12/b.e10.B_RCP8.5_CO2_CAM5.f02_t12.started_2092-12.002/OUTPUT',\n",
    "        'hres.cnt.3':f'{DIR1}/f02_t12/b.e10.B_RCP8.5_CO2_CAM5.f02_t12.started_2092-12.003/OUTPUT',\n",
    "        'hres.cnt.4':f'{DIR1}/f02_t12/b.e10.B_RCP8.5_CO2_CAM5.f02_t12.started_2092-12.004/OUTPUT',\n",
    "        'hres.cnt.5':f'{DIR1}/f02_t12/b.e10.B_RCP8.5_CO2_CAM5.f02_t12.started_2092-12.005/OUTPUT',\n",
    "        'hres.cnt.6':f'{DIR2}/hres_b.e10.B2000_CAM5.f02_t12.started_2092-12_without_SAI.001', # additional run with 6hrly 3D output\n",
    "        'hres.sai.1':f'{DIR2}/hres_b.e10.B2000_CAM5.f02_t12.started_2092-12.001',\n",
    "        'hres.sai.2':f'{DIR2}/hres_b.e10.B2000_CAM5.f02_t12.started_2092-12.002',\n",
    "        'hres.sai.3':f'{DIR2}/hres_b.e10.B2000_CAM5.f02_t12.started_2092-12.003',\n",
    "        'hres.sai.4':f'{DIR2}/hres_b.e10.B2000_CAM5.f02_t12.started_2092-12.004',\n",
    "        'hres.sai.5':f'{DIR2}/hres_b.e10.B2000_CAM5.f02_t12.started_2092-12.005',\n",
    "        'hres.sai.6':f'{DIR2}/hres_b.e10.B2000_CAM5.f02_t12.started_2092-12.006', # additional run with 6hrly 3D output\n",
    "    }\n",
    "\n",
    "    \n",
    "    def __init__(self, tag):\n",
    "        '''Initialize a specific case, identified by its tag.'''\n",
    "        self.tag = tag          # tag, e.g. hres.sai.1\n",
    "        self.directory = self.cases[tag] # casedirectory\n",
    "        if not os.path.isdir(self.directory):\n",
    "            print(f\"directory does not exist: {self.directory}\")\n",
    "        self.name = os.path.basename(self.directory.rstrip('/OUTPUT'))  # casename\n",
    "        self.model_component = None\n",
    "        self.file_stream = None\n",
    "        self.files = self._group_ncFiles()\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        '''Always prints case tag and name. Additionally:\n",
    "          before Case.select(): available file streams for each model component\n",
    "          after Case.select(): additional time info (read first netCDF)\n",
    "        '''\n",
    "        msg = f'{self.tag} -- {self.name}'\n",
    "        if isinstance(self.files, list): # after Case.select()\n",
    "            msg += f'\\nmodel component: {self.model_component}\\nfile stream: {self.file_stream}'\n",
    "            msg += f'\\nfirst file: {self.files[0]}\\nlast file: {self.files[-1]}\\nnumber of files: {len(self.files)}'\n",
    "            msg += self._nc_info()\n",
    "        elif isinstance(self.files, dict): # before Case.select()\n",
    "            msg += f'\\ndirectory: {self.directory}'\n",
    "            for cmp in self.files:\n",
    "                msg += f'\\n{cmp}: {list(self.files[cmp])}'\n",
    "        return msg\n",
    "\n",
    "\n",
    "    def select(self, comp, stream):\n",
    "        '''Select model component and output stream'''\n",
    "        self.model_component = comp\n",
    "        self.file_stream = stream\n",
    "        self.files = self.files[comp][stream]\n",
    "        return self\n",
    "\n",
    "\n",
    "    def open_mfdataset(self, *args, **kwargs):\n",
    "        '''Open netCDF files, wrapper for load_SAIdata.open_mfdataset'''\n",
    "        assert isinstance(self.files, list), 'attempted to open dataset without selecting a model component and file stream'\n",
    "        return open_mfdataset(self.files, *args, **kwargs)\n",
    "\n",
    "\n",
    "    def _nc_info(self):\n",
    "        '''Open a netCDF file and return some basic info'''\n",
    "        try:\n",
    "            with xr.open_dataset(self.files[0]) as ds:\n",
    "                msg = f'\\nsteps in first file: {len(ds.time)}'\n",
    "                if hasattr(ds.time, 'bounds'):\n",
    "                    tbounds = ds[ds.time.bounds]\n",
    "                    bnd_dim = tbounds.dims[tbounds.shape.index(2)]\n",
    "                    step = tbounds.isel(time=0).diff(bnd_dim).dt.total_seconds().item()\n",
    "                    msg += f'\\ntime step in first file: ' + (f'{step/3600:.1f}H' if step<86400 else f'{step/86400:.1f}D' + f' ({ds.time.bounds})')\n",
    "                msg += f'\\ntime in first file: {[str(t) for t in ds.time.data[:3]]} ...'\n",
    "        except Exception as e:\n",
    "            msg = f'Could not fetch additional data from first file due to...\\n{e}'\n",
    "        return msg\n",
    "\n",
    "\n",
    "    def _group_ncFiles(self):\n",
    "        '''Find all netCDF files and group by model component and file stream.'''\n",
    "        result = {comp:{} for comp in self.comps.keys()}\n",
    "        for comp in self.comps:\n",
    "            ncFiles = [os.path.join(root,file) \n",
    "                for root,dirs,files in os.walk(top=os.path.join(self.directory,comp)) \n",
    "                for file in files if file.endswith('.nc')]\n",
    "            for file in ncFiles:\n",
    "                stream = self._process_filename(file)\n",
    "                if stream is None:\n",
    "                    continue\n",
    "                if (stream not in result[comp]) and (stream[0] in ['f','h']): # filter restarts/initial files\n",
    "                    result[comp][stream] = [file]\n",
    "                elif (stream in result[comp]):\n",
    "                    result[comp][stream].append(file)\n",
    "        for comp in result:\n",
    "            result[comp] = dict(sorted(result[comp].items())) # sort streams\n",
    "            for stream in result[comp]:\n",
    "                result[comp][stream] = np.sort(result[comp][stream]).tolist() # sort files\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "    def _process_filename(self, fname):\n",
    "        '''Read filename \"fname\" and return model component and file stream.'''\n",
    "        if ('volcaero' in fname) or ('strataero' in fname):\n",
    "            if '/copy/' in fname:\n",
    "                return None, None\n",
    "            stream = fname.removesuffix('.nc').split('_')[-1].removeprefix('CAM')\n",
    "            if re.match('^feedback-[0-9]{4}$', stream): # group yearly files\n",
    "                stream = 'feedback-YYYY'\n",
    "            return stream\n",
    "        parts = os.path.basename(fname).split('.')\n",
    "        mods = list(self.comps.values()) # model components [cam2, pop, clm2, cice]\n",
    "        try:\n",
    "            imod = [(m in parts) for m in mods].index(True)\n",
    "        except ValueError:\n",
    "            return None, None\n",
    "        is0 = parts.index(mods[imod]) + 1 # stream starts right after model component\n",
    "        for is1 in range(is0,len(parts)): # some filenames have multiple stream parts, e.g. (pop).h.nday1\n",
    "            if parts[is1][0].isnumeric():\n",
    "                break # loop until first numeric part (date) or last part (nc)\n",
    "        if re.match('^avg[0-9]{4}$', parts[is1-1]):\n",
    "            parts[is1-1] = 'avgYYYY' # group yearly averages\n",
    "        stream = '.'.join(parts[is0:is1])\n",
    "        return stream\n",
    "\n",
    "\n",
    "    def _test(self):\n",
    "        '''compare found files with full directory content (using tree)'''\n",
    "        print(self)\n",
    "        print()\n",
    "        if isinstance(self.files, dict): # before Case.select()\n",
    "            for cmp in self.files:\n",
    "                for strm in self.files[cmp]:\n",
    "                    files = self.files[cmp][strm]\n",
    "                    print(f'{\".\".join([cmp,strm])}: {len(files)} files')\n",
    "                    print(files[0].removeprefix(self.directory))\n",
    "                    if len(files)>1:\n",
    "                        print(f'...\\n{files[-1].removeprefix(self.directory)}')\n",
    "                    print()\n",
    "            os.system(f'tree {self.directory}')\n",
    "        elif isinstance(self.files, list): # after Case.select()\n",
    "            print(self.files[0].removeprefix(self.directory))\n",
    "            if len(self.files)>1:\n",
    "                print(f'...\\n{self.files[-1].removeprefix(self.directory)}')\n",
    "            os.system(f'tree {self.directory}/{self.model_component}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49eb073-cb18-4c4b-b9a6-4b241976c10b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
